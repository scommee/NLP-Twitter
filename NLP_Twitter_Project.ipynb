{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import regex as re\n",
        "from nltk.tokenize import word_tokenize\n",
        "!pip install gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "nltk.download('stopwords')  # Download stopwords list if not already present\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assuming 'corpus' is a list of lists, where each inner list contains words from a document\n",
        "stop_words = set(stopwords.words('english'))  # Create a set for efficient lookup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0tkTZIlJ8tw",
        "outputId": "aa575cf9-5ea2-4382-a15d-ae72f09e5ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Embeddings with Twitter Data**"
      ],
      "metadata": {
        "id": "HHHipVxj-8N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"twitter_training.csv\")\n",
        "df['Positive'].value_counts() #Here we can see the different outputs\n",
        "\n",
        "#Create new column with appropriate name\n",
        "df['Sentiment'] = df['Positive']\n",
        "df = df.drop(['Positive', '2401', 'Borderlands'], axis=1)"
      ],
      "metadata": {
        "id": "DmRVn2GXxKFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail(10)"
      ],
      "metadata": {
        "id": "CPhnyLCueD05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={\"im getting on borderlands and i will murder you all ,\":\"Content\"})"
      ],
      "metadata": {
        "id": "5MPn4vaK-7c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Put words in Reviews into Tokens**"
      ],
      "metadata": {
        "id": "W65JOjGnIQg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Content'] = df['Content'].astype(str)\n",
        "df['Content'] = df['Content'].apply(word_tokenize)"
      ],
      "metadata": {
        "id": "CeZ-bFyK_G-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run Word2Vec Model on Data to place into Vectors**"
      ],
      "metadata": {
        "id": "KdhhJoc0IXEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(df['Content'], window=5, vector_size=100, sg=1)"
      ],
      "metadata": {
        "id": "MkI4M8pR_l3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Show each words 100 dimension Vector Space**"
      ],
      "metadata": {
        "id": "sx5X9UZRIvAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = [model.wv[token] for row in df['Content'] for token in row if token in model.wv]\n",
        "\n",
        "#model.wv[token] = any word and its vector space"
      ],
      "metadata": {
        "id": "i-VHcbF8G-iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors"
      ],
      "metadata": {
        "id": "KKp92zFKplZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Calculate Average Vector Position across all Words in Review**"
      ],
      "metadata": {
        "id": "wacx_Qs4I1me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average vector for each review\n",
        "document_vectors = []\n",
        "for row in df['Content']:\n",
        "    row_vectors = [model.wv[token] for token in row if token in model.wv]\n",
        "    if row_vectors:\n",
        "        document_vectors.append(np.mean(row_vectors, axis=0))\n",
        "    else:\n",
        "        document_vectors.append(np.zeros(model.vector_size))  # Handle cases with no valid tokens\n",
        "\n",
        "df['Vectors'] = document_vectors"
      ],
      "metadata": {
        "id": "urIctOjHLU2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Vectors']"
      ],
      "metadata": {
        "id": "mgkqmrWdrVZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "a-gWCtzrjhYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **One-Hot Encoder**"
      ],
      "metadata": {
        "id": "zsDUGH3XI7Vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### USE ONE HOT ENCODER ###\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_data = encoder.fit_transform(df[['Sentiment']])"
      ],
      "metadata": {
        "id": "5-jMt35oVmt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['Sentiment']))\n",
        "y = df['Vectors']\n",
        "X"
      ],
      "metadata": {
        "id": "0zp-OF55kqec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split Data into Train and Test**"
      ],
      "metadata": {
        "id": "6VAgCtkOI-ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3)"
      ],
      "metadata": {
        "id": "1MpJTzRyMck2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run ML model**"
      ],
      "metadata": {
        "id": "dYMQ2NIpJE8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "clf = KNeighborsClassifier()\n",
        "\n",
        "# Assuming y_train contains arrays and you want to extract the first element of each array\n",
        "y_train_updated = np.array([arr[0] for arr in y_train])\n",
        "y_train_updated = np.array([1 if arr[0] > 0.5 else 0 for arr in y_train])\n",
        "clf.fit(X_train, y_train_updated)\n",
        "clf.predict(X_test)\n",
        "\n",
        "# Convert y_test to a NumPy array with binary labels (adjust threshold as needed)\n",
        "y_test_updated = np.array([1 if arr[0] > 0.5 else 0 for arr in y_test])\n",
        "clf.score(X_test, y_test_updated)"
      ],
      "metadata": {
        "id": "AySg6bpAJDTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dimensionality Reduction using PCA**"
      ],
      "metadata": {
        "id": "HvVNUWLVJG80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Dimensionality Reduction for plotting ###\n",
        "\n",
        "vector_data = np.vstack(df['Vectors'].to_numpy())\n",
        "\n",
        "pca = PCA(n_components = 3)\n",
        "\n",
        "threedvec = pca.fit_transform(vector_data)"
      ],
      "metadata": {
        "id": "tNq43i-F1VuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new columns in your DataFrame for each of the 4 components\n",
        "df['threedvec_1'] = threedvec[:, 0]\n",
        "df['threedvec_2'] = threedvec[:, 1]\n",
        "df['threedvec_3'] = threedvec[:, 2]"
      ],
      "metadata": {
        "id": "azJKMMk31YJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plot showing Vector position of Review**"
      ],
      "metadata": {
        "id": "xyrDs2WJJLYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10, 8))  # Adjust figure size as needed\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Define a color palette for the sentiment categories\n",
        "palette = sns.color_palette(\"tab10\", len(df['Sentiment'].cat.categories))\n",
        "\n",
        "# Create the 3D scatter plot with color based on 'Sentiment'\n",
        "scatter = ax.scatter(df['threedvec_1'], df['threedvec_2'], df['threedvec_3'],\n",
        "                     c=df['Sentiment'].cat.codes, cmap=plt.cm.get_cmap('tab10', len(df['Sentiment'].cat.categories)))\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('PCA Component 1')\n",
        "ax.set_ylabel('PCA Component 2')\n",
        "ax.set_zlabel('PCA Component 3')\n",
        "plt.title('Sentiment Visualization (3D)')\n",
        "\n",
        "# Add a legend (optional)\n",
        "handles, labels = scatter.legend_elements()\n",
        "legend = ax.legend(handles, df['Sentiment'].cat.categories, loc=\"upper right\", title=\"Sentiment\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "56Q6onq0Fj0F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}